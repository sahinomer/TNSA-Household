---
title: "TNSA Houselhold Wealth Index"
author: "Ömer Şahin"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Introduction

2013 Turkey Demographic and Health Survey (TDHS-2013), fertility levels and trends, infant and child mortality, family planning and maternal and child health issues a sample survey at the national level designed to provide information on. And also, gives information about the wealth index of the household. In this work, the wealth index analysis and prediction according to other information about a household are evaluated.

# Data Analyze

#### Libraries

Required libraries for data preparation and analyze:
```{r}
library(readr)    # Data loading 
library(tidyr)    # Seperate data column
library(ggplot2)  # Visualize
```

### Preparation

#### Load Household Dataset

Simplified household data of the TNSA dataset is loaded.
```{r}
household <- read.table(file="household.csv", sep=";", header = TRUE, na.strings = c("" , " ","\t ", "Missing" ))

```

Each household row has "case_id" field. These values are unique for each sample. The ID column is dropped due to has no contribution to the training.
```{r}
household <- household[, -1]  # drop case id
```

In house ownership column has only one "Other" feature. Therefore, this row is evaluated as outliers and dropped.
```{r}
household <- household[-which(household$house_ownership == "Other"), ]
```

In this stage, the number of samples:
```{r}
nrow(household)
```

The dataset has some rows with unknowns attributes. The number of rows with missing values is:
```{r}
sum(!complete.cases(household))
```

The number of rows that are not complete can be ignored compared to the total number of samples, and these samples are dropped.
```{r}
household <- na.omit(household)
```

At the end of the data clearing, the number of samples:
```{r}
nrow(household)
```

#### Combined Region

The household data contain the combined region field that consists of combination of a cardinal direction, region, and settlement. These fields are separated into three.
```{r}
head(household$region_combined)

household <- separate(household, region_combined, c("cardinal_direction", "region", "settlement"), sep = '-')
household$cardinal_direction <- as.factor(household$cardinal_direction)
household$region <- as.factor(household$region)
household$settlement <- as.factor(household$settlement)

head(household[, 2:4])
```


#### Wealth Index

The aim of this project is predicting the wealth index of the household. There is an order between wealth index values. Therefore, wealth index factor is releveled. 
```{r}
# Refactor levels of wealth index
household$wealth_index <- factor(household$wealth_index, 
                                 levels = c("Poorest", "Poorer", "Middle", "Richer", "Richest"))
levels(household$wealth_index)
```


#### Attribute Relation

Some attributes have a relation to each other. These relations can be represented as a ratio between them. In this way, all household samples will have attributes that in the same range even they are in the natural number range. After extending columns with the rate of related ones, duplicate columns are dropped. 

```{r}
# Rate of related attributes
household$man_member_rate = (household$household_member - household$woman_member) / household$household_member
household$woman_member_rate = household$woman_member / household$household_member
household$child_member_rate = household$children_under_5 / household$household_member
household$bedroom_rate = household$bedroom_number / household$rooms_number

# Drop duplicated columns with rate values
household <- household[ , -which(names(household) %in% c("woman_member",  # woman_member_rate
                                                         "children_under_5", # children_member_rate
                                                         "bedroom_number"))]  # bedroom_rate
```

#### Data Distribution

The wealth index of a household is the target that is tried to predict. The distribution of the wealth index is quite balanced. It helps to ensure the model does not tend to a class when the target class is balanced. 

```{r}
ggplot(household, aes(x = wealth_index)) + geom_bar() + 
        xlab("Wealth Index") + ylab("Number of Households") + 
        ggtitle(label = "Wealth Index Distribution")
      
```


Distributions of the some of binary attributes about households are too imbalanced. There are not enough counter samples for these attributes. Therefore, these attributes can misguide the model when predicting the wealth index. Imbalanced attributes: 

```{r}
summary(household[, c("refrigerator", "garbage_grinder", "washing_machine", "washer_dryer", 
                      "home_theather", "mobile_phone", "taxi_minibus", "tractor", "motorcycle")])
```

According to results, nearly all households have the refrigerator, washing machine, and mobile phone. On the other hand, nearly none of the households have the garbage grinder, washer dryer or home theatre. As a result, these attributes are not used for training the model.

```{r}
household <- household[ , -which(names(household) %in% c("refrigerator", "garbage_grinder",
                                                         "washing_machine", "washer_dryer", 
                                                         "home_theather", "mobile_phone",
                                                         "taxi_minibus", "tractor", "motorcycle"))]
```

Province column in dataset is also imbalanced and has too many different classes due to there are 81 cities in Turkey. Besides, the Random Forest model cannot handle too many categories. Therefore, the province column is dropped to evaluate other models in equal conditions.

```{r}
summary(household$province)
household <- household[, -which(names(household) == "province")]
```

#### Outliers

TNSA Household dataset was collected under controlled by qualified observers. Therefore, Obtained information about households is accepted as truth. Information that does not have enough observations or balanced distribution is not used in the analysis to make a proper evaluation.

## Analyze


```{r eval=FALSE, include=FALSE}
# Plot distribution of data
# for(colnm in colnames(household)) {
#   print(ggplot(household, aes_string(x = colnm)) +
#           geom_bar() + ylab("Number of Households"))
# }

# for(colmn in colnames(household)) {
#   tbl <- table(household$wealth_index, household[,colmn])
#   tbl <- tbl / rowSums(tbl)
#   conf_matrix <- as.data.frame(tbl)
#   print(
#     ggplot(data =  conf_matrix, mapping = aes(x = Var1, y = Var2)) +
#       xlab("wealth_index") + ylab(colmn) + 
#       geom_tile(aes(fill = Freq)) +
#       geom_text(aes(label = sprintf("%0.4f", Freq)), vjust = 1) +
#       scale_fill_gradient(low = "blue",
#                           high = "red",
#                           trans = "log")
#   )
# }
```


# Classification

Each sample row contains 34 features about household and wealth index. The wealth index of the household is the target value that is tried to predict by the classifier. Different types of classifiers are compared with others and itself that uses a different parameter set.

#### Libraries

Required libraries for data preparation and analyze:
```{r message=FALSE, warning=FALSE}
library(performanceEstimation)   # Performance estimation

library(CORElearn)     # Feature extraction
library(DMwR2)         # Decision tree
library(rpart)         # Decision tree
library(rpart.plot)    # Decision tree Visualize
library(e1071)         # Support vector machine
library(randomForest)  # Random forest
library(adabag)        # Adabag
library(h2o)           # Neural network
```

#### Split Train/Test Set

Randomly selected 20 percent of the dataset is used as a test and the rest of them are used for training models. 

```{r}
set.seed(1024)
samples <- sample(1:nrow(household), nrow(household)*0.8)
train <- household[samples, ]
test <- household[-samples, ]
```

#### Initialize Result Vector

Initialize results vector to compare different models according to their best.

```{r}
results <- c(0, 0, 0, 0, 0)
names(results) <- c("Decision Tree", "Support Vector Machine", 
                              "Random Forest", "Adabag", "Neural Network")

```


### Decision Tree

The first tried model is the decision tree. Optimum parameters are selected by using a performance estimator. The model is trained with this parameter set and the test set is predicted by the model.
Parameters are information gain threshold and prune value. Features are sorted by their information gain and thresholded by a value to reduce the number of features. Features are kept if their information gain value greater than the threshold. Decision tree pruning is evaluated by given parameters.

Paramter set:
```{r}
inf_gain <- c(0.0, 0.1, 0.2)
prune_se <- c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)
```


```{r cache=TRUE}
load("decisiontree.perf")  # Load pre-estimated performances
plot(perfEst)   # Plot performance estimation results

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

inf_gain <- work_flow@pars[["inf_gain"]]
prune_se <- work_flow@pars[["prune_se"]]

cat("Best Paramter set:",
    "\n  inf_gain : ", inf_gain,
    "\n  prune_se : ", prune_se)

# Train model with best parameter set for testing
information_gain <- attrEval(wealth_index ~ ., train, estimator = "InfGain")
features <- names(information_gain)[information_gain>inf_gain]

model <- rpartXse(wealth_index ~ ., train[,c("wealth_index", features)], se = prune_se)
predicted <- predict(model, test, type = "class")

conf_matrix <- table(predicted, test$wealth_index)
conf_matrix
results["Decision Tree"] <- sum(diag(conf_matrix)) / sum(conf_matrix)
results["Decision Tree"]

```



### Support Vector Machine
SVM---

Paramter set:
```{r}
cost <- c(1, 5, 10)
gamma <- c(0.01, 0.001)
```

```{r cache=TRUE}
load("svm.perf")  # Load pre-estimated performances
plot(perfEst)   # Plot performance estimation results

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

cost <- work_flow@pars[["learner.pars"]][["cost"]]
gamma <- work_flow@pars[["learner.pars"]][["gamma"]]

cat("Best Paramter set:",
    "\n  cost  : ", cost,
    "\n  gamma : ", gamma)

# Train model with best parameter set for testing
model <- svm(wealth_index ~ ., train, cost = cost, gamma = gamma)
predicted <- predict(model, test, type = "class")

conf_matrix <- table(predicted, test$wealth_index)
conf_matrix
results["Support Vector Machine"] <- sum(diag(conf_matrix)) / sum(conf_matrix)
results["Support Vector Machine"]
```



### Random Forest
-- random forest

Paramter set:
```{r}
cost <- c(1, 5, 10)
gamma <- c(0.01, 0.001)
```

```{r cache=TRUE}
load("randomforest.perf")  # Load pre-estimated performances
plot(perfEst) 

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

ntree <- work_flow@pars[["learner.pars"]][["ntree"]]

cat("Best Paramter set:",
    "\n  ntree  : ", ntree)

# Train model with best parameter set for testing
model <- randomForest(wealth_index ~ ., train, ntree = ntree)

predicted <- predict(model, test, type = "class")

conf_matrix <- table(predicted, test$wealth_index)
conf_matrix
results["Random Forest"] <- sum(diag(conf_matrix)) / sum(conf_matrix)
results["Random Forest"]
```



### Adabag
--- adabagg

Paramter set:
```{r}
cost <- c(1, 5, 10)
gamma <- c(0.01, 0.001)
```

```{r cache=TRUE}
load("adabag.perf")  # Load pre-estimated performances
plot(perfEst) 

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

mfinal <- work_flow@pars[["mfinal"]]
control <- work_flow@pars[["control"]]

cat("Best Paramter set:",
    "\n  mfinal  : ", mfinal,
    "\n  control : ", control$maxdepth)

# Train model with best parameter set for testing
model <- bagging(wealth_index ~ ., train, mfinal = mfinal, control = control)

predicted <- predict(model, test, type = "class")

predicted$confusion
results["Adabag"] <- sum(diag(predicted$confusion)) / sum(predicted$confusion)
results["Adabag"]
```


### Results

```{r}
df = data.frame(Model = names(results), 
                Accuracy = results)
ggplot(df, aes(x = Model, weight=Accuracy)) + geom_bar() + 
        xlab("Model") + ylab("Accuracy") + 
        ggtitle(label = "Model Accuracies")
```










