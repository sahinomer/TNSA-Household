---
title: "TNSA Household Wealth Index"
author: "Ömer Şahin"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Introduction

2013 Turkey Demographic and Health Survey (TDHS-2013), fertility levels and trends, infant and child mortality, family planning and maternal and child health issues a sample survey at the national level designed to provide information on. And also, gives information about the wealth index of the household. In this work, the wealth index analysis and prediction according to other information about a household are evaluated.

# Data Analyze

#### Libraries

Required libraries for data preparation and analyze:
```{r}
library(readr)    # Data loading 
library(tidyr)    # Seperate data column
library(ggplot2)  # Visualize
```

#### Load Household Dataset

Simplified household data of the TNSA dataset is loaded.
```{r}
household <- read.table(file="household.csv", sep=";", header = TRUE, na.strings = c("" , " ","\t ", "Missing" ))

```

Each household row has "case_id" field. These values are unique for each sample. The ID column is dropped due to has no contribution to the training.
```{r}
household <- household[, -1]  # drop case id
```

In house ownership column has only one "Other" feature. Therefore, this row is evaluated as outliers and dropped.
```{r}
household <- household[-which(household$house_ownership == "Other"), ]
household$house_ownership <- factor(household$house_ownership)
```

In this stage, the number of samples:
```{r}
nrow(household)
```

The dataset has some rows with unknown attributes. The number of rows with missing values is:
```{r}
sum(!complete.cases(household))
```

The number of rows that are not complete can be ignored compared to the total number of samples, and these samples are dropped.
```{r}
household <- na.omit(household)
```

At the end of the data clearing, the number of samples:
```{r}
nrow(household)
```

#### Combined Region

The household data contain the combined region field that consists of a combination of a cardinal direction, region, and settlement. These fields are separated into three.
```{r}
head(household$region_combined)

household <- separate(household, region_combined, c("cardinal_direction", "region", "settlement"), sep = '-')
household$cardinal_direction <- as.factor(household$cardinal_direction)
household$region <- as.factor(household$region)
household$settlement <- as.factor(household$settlement)

head(household[, 2:4])
```


#### Wealth Index

The aim of this project is predicting the wealth index of the household. There is an order between wealth index values. Therefore, wealth index factor is releveled. 
```{r}
# Refactor levels of wealth index
household$wealth_index <- factor(household$wealth_index, 
                                 levels = c("Poorest", "Poorer", "Middle", "Richer", "Richest"))
levels(household$wealth_index)
```


#### Attribute Relation

Some attributes have a relation to each other. These relations can be represented as a ratio between them. In this way, all household samples will have attributes that in the same range even they are in the natural number range. After extending columns with the rate of related ones, duplicate columns are dropped. 

```{r}
# Rate of related attributes
household$man_member_rate = (household$household_member - household$woman_member) / household$household_member
household$woman_member_rate = household$woman_member / household$household_member
household$child_member_rate = household$children_under_5 / household$household_member
household$bedroom_rate = household$bedroom_number / household$rooms_number

# Drop duplicated columns with rate values
household <- household[ , -which(names(household) %in% c("woman_member",  # woman_member_rate
                                                         "children_under_5", # children_member_rate
                                                         "bedroom_number"))]  # bedroom_rate
```

#### Data Distribution

The wealth index of a household is the target that is tried to predict. The distribution of the wealth index is quite balanced. It helps to ensure the model does not tend to a class when the target class is balanced. 

```{r}
ggplot(household, aes(x = wealth_index)) + geom_bar() + 
        xlab("Wealth Index") + ylab("Number of Households") + 
        ggtitle(label = "Wealth Index Distribution")
      
```


Distributions of the some of binary attributes about households are too imbalanced. There are not enough counter samples for these attributes. Therefore, these attributes can misguide the model when predicting the wealth index. Imbalanced attributes: 

```{r}
summary(household[, c("refrigerator", "garbage_grinder", "washing_machine", "washer_dryer", 
                      "home_theather", "mobile_phone", "taxi_minibus", "tractor", "motorcycle")])
```

According to results, nearly all households have a refrigerator, washing machine, and a mobile phone. On the other hand, nearly none of the households have the garbage grinder, washer dryer or home theatre. As a result, these attributes are not used for training the model.

```{r}
household <- household[ , -which(names(household) %in% c("refrigerator", "garbage_grinder",
                                                         "washing_machine", "washer_dryer", 
                                                         "home_theather", "mobile_phone",
                                                         "taxi_minibus", "tractor", "motorcycle"))]
```

Province column in the dataset is also imbalanced and has too many different classes due to there are 81 cities in Turkey. Besides, the Random Forest model cannot handle too many categories. Therefore, the province column is dropped to evaluate other models in equal conditions.

```{r}
head(summary(household$province))
household <- household[, -which(names(household) == "province")]
```

#### Outliers

TNSA Household dataset was collected under controlled by qualified observers. Therefore, Obtained information about households is accepted as truth. Information that does not have enough observations or balanced distribution is not used in the analysis to make a proper evaluation.

## Analyze

At the end of data preparation, data analyzing by visualizing is done over some information about the household dataset.


The relation between wealth index and settlement area:
```{r echo=FALSE}
tbl <- table(household$wealth_index, household$settlement)
tbl <- tbl / rowSums(tbl)
conf_matrix <- as.data.frame(tbl)
ggplot(data =  conf_matrix, mapping = aes(x = Var1, y = Var2)) +
   xlab("Wealth Index") + ylab("Settlement") + 
   geom_tile(aes(fill = Freq)) +
   geom_label(aes(label = sprintf("%0.4f", Freq)), vjust = 0.5) +
   scale_fill_gradient(low = "white",
                       high = "blue")
```

Most of the people live in urban areas. On the other hand, when the wealth index is reducing, people tend to live in rural areas or people that live in rural areas are poorer than the others.


The relation between wealth index and house ownership:
```{r echo=FALSE}
tbl <- table(household$wealth_index, household$house_ownership)
tbl <- tbl / rowSums(tbl)
conf_matrix <- as.data.frame(tbl)
ggplot(data =  conf_matrix, mapping = aes(x = Var1, y = Var2)) +
   xlab("Wealth Index") + ylab("House Ownership") + 
   geom_tile(aes(fill = Freq)) +
   geom_label(aes(label = sprintf("%0.4f", Freq)), vjust = 0.5) +
   scale_fill_gradient(low = "white",
                       high = "blue")
```

Contrary to expectation, poor households live in their own houses and richer households may prefer to live in a rented house.


The relation between wealth index and owning another house:
```{r echo=FALSE}
tbl <- table(household$wealth_index, household$owns_another_house)
tbl <- tbl / rowSums(tbl)
conf_matrix <- as.data.frame(tbl)
ggplot(data =  conf_matrix, mapping = aes(x = Var1, y = Var2)) +
   xlab("Wealth Index") + ylab("Own Another House") + 
   geom_tile(aes(fill = Freq)) +
   geom_label(aes(label = sprintf("%0.4f", Freq)), vjust = 0.5) +
   scale_fill_gradient(low = "white",
                       high = "blue")

```

According to the data, people do not much interest in having another house. Still, rich households much more have another house than the poor ones as expected.


The relation between internet connection and wealth index:
```{r echo=FALSE}
tbl <- table(household$wealth_index, household$internet)
tbl <- tbl / rowSums(tbl)
conf_matrix <- as.data.frame(tbl)
ggplot(data =  conf_matrix, mapping = aes(x = Var1, y = Var2)) +
   xlab("Wealth Index") + ylab("Internet Connection") + 
   geom_tile(aes(fill = Freq)) +
   geom_label(aes(label = sprintf("%0.4f", Freq)), vjust = 0.5) +
   scale_fill_gradient(low = "white",
                       high = "blue")

```

According to 2013 data, the internet connection is still not very prevalent and seen as a luxury feature. 



# Classification

Each sample row contains 34 features about household and wealth index. The wealth index of the household is the target value that is tried to predict by the classifier. Different types of classifiers are compared with others and itself that uses a different parameter set.

#### Libraries

Required libraries for data classification and performace evaluation:
```{r message=FALSE, warning=FALSE}
library(performanceEstimation)   # Performance estimation
library(caret)         # Confusion matrix
library(CORElearn)     # Feature extraction
library(DMwR2)         # Decision tree
library(rpart)         # Decision tree
library(rpart.plot)    # Decision tree visualize
library(e1071)         # Support vector machine
library(randomForest)  # Random forest
library(adabag)        # Adabag
library(h2o)           # Neural network
```


#### Split Train/Test Set

Randomly selected 20 percent of the dataset is used as a test and the rest of them are used for training models. 

```{r}
set.seed(1024)
samples <- sample(1:nrow(household), nrow(household)*0.8)
train <- household[samples, ]
test <- household[-samples, ]
```

### Analyze Decision Boundary

By using a simple decision tree, tried to understand the decision boundary for classification household dataset according to the wealth index.

```{r warning=FALSE}
decision_tree <- rpartXse(wealth_index ~ ., 
                          train[,c("wealth_index", "heating", "dishwasher", 
                                   "internet", "settlement")], se = 1)

predicted <- predict(decision_tree, test, type = "class")

conf_matrix <- confusionMatrix(predicted, test$wealth_index)
conf_matrix$table
conf_matrix$overall["Accuracy"]

rpart.plot(decision_tree)

```

Asking a household member about settlement area that they are living, their heating method, whether they have dishwasher and internet connection gives an idea of the wealth index of the household by mostly misclassified only one level different. After all, a better classification model is required for more success. Even attributes that have high information gain are selected, the wealth index cannot be predicted directly by using some partition of attributes.


```{r include=FALSE}
results <- c(0, 0, 0, 0, 0)
names(results) <- c("Decision Tree", "Support Vector Machine", 
                              "Random Forest", "Adabag", "Neural Network")

```

### Decision Tree

The first model is the decision tree. Optimum parameters are selected by using a performance estimator. The model is trained with this parameter set and the test set is predicted by the model.
Parameters are information gain threshold and prune value. Features are sorted by their information gain and thresholded by a value to reduce the number of features. Features are kept if their information gain value is greater than the threshold. Decision tree pruning is evaluated by given parameters.

Parameter set:
```{r}
inf_gain <- c(0.0, 0.1, 0.2)
prune_se <- c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)
```


```{r decisiontree, cache=TRUE}
load("decisiontree.perf")  # Load pre-estimated performances
plot(perfEst)   # Plot performance estimation results

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

inf_gain <- work_flow@pars[["inf_gain"]]
prune_se <- work_flow@pars[["prune_se"]]

cat("Best Parameter set:",
    "\n  inf_gain : ", inf_gain,
    "\n  prune_se : ", prune_se)

# Train model with best parameter set for testing
information_gain <- attrEval(wealth_index ~ ., train, estimator = "InfGain")
features <- names(information_gain)[information_gain>inf_gain]

model <- rpartXse(wealth_index ~ ., train[,c("wealth_index", features)], se = prune_se)

predicted <- predict(model, test, type = "class")

conf_matrix <- confusionMatrix(predicted, test$wealth_index)
conf_matrix$table
results["Decision Tree"] <- conf_matrix$overall["Accuracy"]
results["Decision Tree"]

```



### Support Vector Machine
SVM creates a decision boundary between classes. Cost and gamma parameters are evaluated by performance estimation. The test model is trained with the optimum parameter set for the classification.

Parameter set:
```{r}
cost <- c(1, 5, 10)
gamma <- c(0.01, 0.001)
```

```{r svm, cache=TRUE}
load("svm.perf")  # Load pre-estimated performances
plot(perfEst)   # Plot performance estimation results

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

cost <- work_flow@pars[["learner.pars"]][["cost"]]
gamma <- work_flow@pars[["learner.pars"]][["gamma"]]

cat("Best Parameter set:",
    "\n  cost  : ", cost,
    "\n  gamma : ", gamma)

# Train model with best parameter set for testing
model <- svm(wealth_index ~ ., train, cost = cost, gamma = gamma)

predicted <- predict(model, test, type = "class")

conf_matrix <- confusionMatrix(predicted, test$wealth_index)
conf_matrix$table
results["Support Vector Machine"] <- conf_matrix$overall["Accuracy"]
results["Support Vector Machine"]
```


### Adabag
Bootstrap Aggregation (Bagging) is an ensemble method. The number of iterations for boosting run and max depth is evaluated by using the performance estimator.

Parameter set:
```{r}
mfinal = c(20, 80, 120, 200)
maxdepth = c(5, 10, 20)
```

```{r adabag, cache=TRUE}
load("adabag.perf")  # Load pre-estimated performances
plot(perfEst) 

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

mfinal <- work_flow@pars[["mfinal"]]
maxdepth <- work_flow@pars[["maxdepth"]]

cat("Best Parameter set:",
    "\n  mfinal   : ", mfinal,
    "\n  maxdepth : ", maxdepth)

# Train model with best parameter set for testing
model <- bagging(wealth_index ~ ., train, mfinal = mfinal, control = rpart.control(maxdepth=maxdepth))

predicted <- predict(model, test, type = "class")

conf_matrix <- confusionMatrix(as.factor(predicted$class), test$wealth_index)
conf_matrix$table
results["Adabag"] <- conf_matrix$overall["Accuracy"]
results["Adabag"]
```

### Random Forest
Random Forest is a combination of the multiple decision trees to prevent overfitting that comes with decision trees. The number of trees is evaluated by using the performance estimator.

Parameter set:
```{r}
ntree = c(100, 200, 300, 400, 500)
```

```{r randomforest, cache=TRUE}
load("randomforest.perf")  # Load pre-estimated performances
plot(perfEst) 

# Best parameter set
work_flow <- getWorkflow(topPerformers(perfEst, maxs = TRUE)[[1]][1,1], perfEst)

ntree <- work_flow@pars[["learner.pars"]][["ntree"]]

cat("Best Parameter set:",
    "\n  ntree  : ", ntree)

# Train model with best parameter set for testing
model <- randomForest(wealth_index ~ ., train, ntree = ntree)

predicted <- predict(model, test, type = "class")

conf_matrix <- confusionMatrix(predicted, test$wealth_index)
conf_matrix$table
results["Random Forest"] <- conf_matrix$overall["Accuracy"]
results["Random Forest"]
```



### Neural Network
A neural network is designed that is suitable for the classification problem. The architecture of the neural network consists of three hidden layers. After each hidden layer dropout is applied to prevent overfitting and ReLu is used as an activation function.

```{r eval=FALSE, include=FALSE}
h2oInstance <- h2o.init(ip = "localhost") # start H2O instance locally
```

```{r ann, cache=TRUE, results="hide"}
train_frame <- as.h2o(train, "train_frame")
test_frame <- as.h2o(test, "test_frame")

model <- h2o.deeplearning(x=names(household[, -1]), 
                          y=c("wealth_index"), 
                          training_frame=train_frame,
                          hidden = c(196, 256, 128),
                          hidden_dropout_ratios = c(0.4, 0.4, 0.4),
                          activation = "RectifierWithDropout",
                          epochs = 500,
                          shuffle_training_data=TRUE)

predicted <- as.data.frame(h2o.predict(model, test_frame))

```


```{r}
predicted$predict <- factor(predicted$predict, 
                            levels = c("Poorest", "Poorer", "Middle", "Richer", "Richest"))

predicted$target <- test$wealth_index

conf_matrix <- confusionMatrix(predicted$predict, predicted$target)
conf_matrix$table
results["Neural Network"] <- conf_matrix$overall["Accuracy"]
results["Neural Network"]
```

```{r eval=FALSE, include=FALSE}
h2o.shutdown(prompt = F)
```


# Results

The success of models that are trained with optimal parameter sets are given in the following bar plot:

```{r}
df = data.frame(Model = names(results), 
                Accuracy = results*100)
ggplot(df, aes(x = Model, weight=Accuracy)) + geom_bar() + 
        xlab("Model") + ylab("Accuracy") + 
        ggtitle(label = "Model Accuracies") + 
        geom_text(aes(label = sprintf("%0.4f", round(Accuracy, digits = 4)), 
                      y = Accuracy-5), size = 3)
```


According to results, Adabag achieved the worst score, decision tree, and random forest got more success than bagging. SVM and neural network models achieved a similar score on classification TNSA household data for the wealth index. The neural network model is the best in evaluated ones. Besides, misclassification is done mostly in favor of the close class such as richer to richest.

The normalized confusion matrix of the neural network that is the best model is given at the following plot:

```{r warning=FALSE}
normalized_conf_matrix <- conf_matrix$table / colSums(conf_matrix$table)
normalized_conf_matrix <- as.data.frame(normalized_conf_matrix)

ggplot(data =  normalized_conf_matrix, mapping = aes(x = Reference, y = Prediction)) +
       xlab("Reference") + ylab("Prediction") + 
       geom_tile(aes(fill = Freq)) +
       geom_label(aes(label = sprintf("%0.4f", Freq)), vjust = 0.5) +
       scale_fill_gradient(low = "white",
                           high = "blue")
```


According to the confusion matrix, the neural network model is more successful in predicting the extreme classes than the predicting to wealth indexes in the middle.





